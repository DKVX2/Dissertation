{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries and data**\n"
      ],
      "metadata": {
        "id": "_N9X6-wCJGhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOcKIDMJIRou"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets scikit-learn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load datasets\n",
        "urc_llm_train = pd.read_csv('urc_llm_train.csv')\n",
        "urc_llm_validation = pd.read_csv('urc_llm_validation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenise the Data**"
      ],
      "metadata": {
        "id": "T1HUXeDtJp8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds before initializing anything that uses randomness\n",
        "seed_value = 12345\n",
        "torch.manual_seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a function to tokenize and encode data\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the articles\n",
        "    tokenized = tokenizer(examples['Article'], padding=\"max_length\", truncation=True)\n",
        "    # Add encoded labels (Solidarity column)\n",
        "    tokenized['label'] = examples['Solidarity']\n",
        "    return tokenized\n",
        "\n",
        "# Convert DataFrames to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(urc_llm_train)\n",
        "val_dataset = Dataset.from_pandas(urc_llm_validation)\n",
        "\n",
        "# Apply the preprocessing function\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "p8VUbQhXJrTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model for binary classification\n",
        "num_labels = 2\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    learning_rate=1e-5,              # Learning Rate\n",
        "    lr_scheduler_type='cosine',\n",
        "    num_train_epochs=30,             # Number of training epochs\n",
        "    per_device_train_batch_size=4,   # Batch size for training\n",
        "    per_device_eval_batch_size=4,    # Batch size for evaluation\n",
        "    warmup_steps=1000,                # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # Strength of weight decay\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"  # Evaluate after each epoch\n",
        ")\n",
        "\n",
        "# Define accuracy metric for binary classification\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='binary', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='binary')\n",
        "    f1 = f1_score(labels, preds, average='binary')\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcr-L5JeJ8VE",
        "outputId": "6ca12a6e-d7b2-4445-fbd4-c160f100d256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model and validation results**"
      ],
      "metadata": {
        "id": "PGNa5gAwK29e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The model to train\n",
        "    args=training_args,                  # Training arguments\n",
        "    train_dataset=train_dataset,         # Training dataset\n",
        "    eval_dataset=val_dataset,            # Validation dataset\n",
        "    compute_metrics=compute_metrics      # Function to compute metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"Validation results:\", eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lfMg4oDKK4Hz",
        "outputId": "64195cf1-922d-4342-fed7-3b03aa8d6fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='990' max='990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [990/990 12:58, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.668200</td>\n",
              "      <td>0.640960</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.648900</td>\n",
              "      <td>0.622156</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.602900</td>\n",
              "      <td>0.600494</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.601200</td>\n",
              "      <td>0.581322</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.609800</td>\n",
              "      <td>0.567693</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.590900</td>\n",
              "      <td>0.553973</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.542651</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.586300</td>\n",
              "      <td>0.534390</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.596100</td>\n",
              "      <td>0.521496</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.465800</td>\n",
              "      <td>0.511671</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.522500</td>\n",
              "      <td>0.501787</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.864865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.480600</td>\n",
              "      <td>0.495540</td>\n",
              "      <td>0.773810</td>\n",
              "      <td>0.771084</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.870748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.402600</td>\n",
              "      <td>0.499466</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.782051</td>\n",
              "      <td>0.953125</td>\n",
              "      <td>0.859155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.272100</td>\n",
              "      <td>0.539367</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.786885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.399100</td>\n",
              "      <td>0.560031</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.841270</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.834646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.237400</td>\n",
              "      <td>0.590754</td>\n",
              "      <td>0.797619</td>\n",
              "      <td>0.830986</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.874074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.186300</td>\n",
              "      <td>0.639022</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.830769</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.837209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.143900</td>\n",
              "      <td>1.007986</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.718750</td>\n",
              "      <td>0.793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.152400</td>\n",
              "      <td>1.002814</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>0.806452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>1.305259</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.718750</td>\n",
              "      <td>0.793103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>1.190008</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.841270</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.834646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>1.214273</td>\n",
              "      <td>0.773810</td>\n",
              "      <td>0.835821</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.854962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.026200</td>\n",
              "      <td>1.332698</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.841270</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.834646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>1.424638</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.825397</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.818898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>1.482681</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.847458</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>0.813008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>1.531467</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>0.806452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>1.625113</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>0.796610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.005200</td>\n",
              "      <td>1.723184</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.895833</td>\n",
              "      <td>0.671875</td>\n",
              "      <td>0.767857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>1.744646</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.859649</td>\n",
              "      <td>0.765625</td>\n",
              "      <td>0.809917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>1.769569</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.847458</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>0.813008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation results: {'eval_loss': 0.5907542705535889, 'eval_accuracy': 0.7976190476190477, 'eval_precision': 0.8309859154929577, 'eval_recall': 0.921875, 'eval_f1': 0.874074074074074, 'eval_runtime': 2.6261, 'eval_samples_per_second': 31.986, 'eval_steps_per_second': 7.997, 'epoch': 30.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run predictions on test data**"
      ],
      "metadata": {
        "id": "wYeQi6V6R6h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best trained model\n",
        "best_checkpoint_path = trainer.state.best_model_checkpoint\n",
        "print(\"Best checkpoint:\", best_checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVk7hRTeR8GT",
        "outputId": "c3a6966b-c526-4cdc-dc62-b8f554b0700c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best checkpoint: ./results/checkpoint-528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = './results/checkpoint-528'\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "6j_TWFicR-bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_len):\n",
        "        self.texts = texts.tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "Of15znXnWbKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('urc_official_test2.csv')\n",
        "test_texts = test_data['Article']\n",
        "\n",
        "# Create a dataset for the test data\n",
        "test_dataset = TestDataset(\n",
        "    texts=test_texts,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=512\n",
        ")"
      ],
      "metadata": {
        "id": "-wGxRE1WWcUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for the test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=4)\n",
        "\n",
        "# Run predictions on the test data\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        predictions.extend(preds.cpu().numpy())"
      ],
      "metadata": {
        "id": "DCpVBdBlWmba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['solidarity_predictions'] = predictions"
      ],
      "metadata": {
        "id": "C1OTZ50JWt6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv('urc_llm_solidarity_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "do2poVsXWuh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}